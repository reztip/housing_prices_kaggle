---
title: "house_prices"
author: "Alex P."
date: "December 30, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
library(plyr)
library(dplyr)
library(knitr)
library(reshape2)
library(dummies)
library(randomForest)
```

## The Problem

A house is usually the most expensive purchase an individual makes in his or her lifetime.  This is why it's so critical that you are able to get a fair deal on a property - overpaying by $100,000 is a fatal mistake!

In this analysis, I build a regression model using data on houses in Ames, Iowa.  The dataset has 79 explanatory variables usable to predict home selling prices.  Home buyers may not be building regression models to evaluate their home purchases, but appraisers, real estate firms investment firms can use these techniques to evaluate fair property values.

## Basic Data Preprocessing

```{r}
setwd("/home/reztip/Desktop/kaggle/house_prices/source")
root = ".."
data.path  =  file.path(root, "input", "train.csv")
train.df = read.csv(data.path, header = TRUE)
```

```{r}
house.ids = dplyr::select(train.df, Id)
house.prices = dplyr::select(train.df, SalePrice)
# Drop the id from training
train.df = dplyr::select(train.df, -Id)
```

##Analysis of Dataset Structure

How many variables do we have for training? How many houses does this dataset cover?
```{r}
# How many variables we have for training
length(names(train.df)) - 1
nrow(train.df)
```
  
How many of these data are numeric? At first glance, it looks to be about half. However, this dataset doesn't look very "clean" - lots of these features are categorical which requires the generation of more binary features (one column for each value of a categorical variable). More on that later.

```{r}
colwise(is.numeric)(train.df) %>% data.frame %>% 
  melt(id = "SalePrice") %>% select(variable, value)
colwise(is.numeric)(train.df) %>% unlist %>% sum
```


How about data quality - which variables are missing the most data? 

```{r}
unlist(colwise(function(x) sum(is.na(x)))(train.df)) %>% sort(decreasing = T)
```

We are missing the most data for POOLQC, MiscFeature, Alley and Fence - more than $2/3$ of the dataset. I drop these variables.

```{r}
train.df = dplyr::select(train.df, -PoolQC, -MiscFeature, -Alley, -Fence)
```

## Dealing With Missing Values

I also do either median filling or mode filling for the other variables missing any data.

```{r}
nas.meaningful = c("FireplaceQu", "GarageType", "GarageYrBlt", "GarageFinish",
                   "GarageQual", "GarageCond", "BsmtExposure", "BsmtFinType2",
                   "BsmtQual", "BsmtCond", "BsmtFinType1")

nas.not.meaningful = c("LotFrontage", "MasVnrType", "MasVnrArea", "Electrical")
```

Encoding of a new variable for data where NA is actually meaningful information.

```{r}
for( var in nas.meaningful){
  fact.levels = levels(train.df[, var])
  mode.val = "Meaningful.NA"
  train.df[var] = factor(train.df[,var], 
                  levels = c(fact.levels, mode.val))
  train.df[is.na(train.df[,var]), var] = mode.val
}
```

Mode filling for categorical variables.

```{r}
for(var in nas.not.meaningful){
  if(! is.numeric(train.df[, var])){
    fact.levels = levels(train.df[, var])
    mode.val = sort(table(train.df[,var]), decreasing = T)[[1]]
    train.df[,var] = factor(train.df[,var], 
                  levels = c(fact.levels, mode.val))
  }
  else { 
    mode.val = median(train.df[,var], na.rm = T)
  }
    train.df[is.na(train.df[,var]), var] = mode.val
}
```

This is all well and good, but there is a lot of information still here that requires manual scrubbing. Take, for example, the *MSSubClass* variable.  It is recorded as a numeric variable, but really represents categorical information.

I "fix" dataset problems in the code below.

```{r}
train.df$MSSubClass = as.factor(train.df$MSSubClass)
train.df$OverallQual = as.factor(train.df$OverallQual)
train.df$OverallCond = as.factor(train.df$OverallCond)
train.df$GarageYrBlt = NULL
```

I also remove variables for which we only have one value of the factor.

```{r}
uniq.table = sapply(train.df, function(x) length(unique(x))) %>%
  sort(decreasing = TRUE)
bad.names = uniq.table[uniq.table == 1] %>% names
# Remove bad names
for(bn in bad.names) {
  train.df[, bn] = NULL
}
```

I drop the YrSold variable as well, since it does not appear to be correlated to saleprice.
```{r}
train.df$YrSold = NULL
```

Now we can being some analysis.

# Data Visualization

First, how are home prices distributed?

```{r}
ggplot(train.df, 
       aes(SalePrice, y = ..density.., fill = I("floralwhite"))) +
  geom_histogram(bins = 30, color = 'black') + 
  scale_x_continuous(labels = scales::dollar_format()) + 
  labs(title = "Distribution of Home Selling Prices")
```

There is a clear trend here, the dataset looks like a log-powernormal distribution. There is a clear mode at just below $200,000, with a short left tail and a very long right tail. There is a pretty high standard deviation to the dataset - about $79,000!

There appears to be a "power law" at play, indicating that the logarithm of home price will probably look more gaussian.

```{r}
ggplot(train.df, 
       aes(SalePrice, y = ..density.., fill = I("floralwhite"))) +
  geom_histogram(bins = 30, color = 'black') + 
  scale_x_log10(labels = scales::dollar_format(), 
                breaks = c(.5E5, 1E5, 2E5, 5E5)) + 
  labs(title = "Distribution of Home Selling Prices (Log-Scaled)")
```

Because the distribution of log price appears relatively gaussian, we may want to predict log-price to better fit regression model theory. More on that later (potentially).

Let's see the distribution of when houses were built.

```{r}
ggplot(train.df, aes(YearBuilt)) + geom_histogram(bins = 30) +
  labs(title = "Distribution of Home Building Year")
```

There was a huge glut of  houses built in the 60s and 70s, as well as the 2000s. How does this correlate to housing prices?

```{r}
ggplot(train.df, aes(YearBuilt, SalePrice)) + geom_point() +
  labs(title = "Scatterplot of Home Building Year vs Price" ) + 
  scale_y_continuous( labels = scales::dollar_format())
```

There is slight upward trend in housing prices relating to the year built, with a significant group of very expensive in the latter years - post 2000.

```{r}
late.years = train.df$YearBuilt > 2000
train.df$late.years = late.years
```

Generate dummies for factor variables.

```{r}
facts = colwise(is.factor)(train.df)
facts = names(train.df)[unlist(facts)]
for(fact in facts){
  nm = train.df[, fact] %>% table %>% names
  nm = paste(fact, nm, sep="")
  dframe = data.frame(dummy(train.df [ ,fact]))
  names(dframe) = nm
  train.df = cbind(train.df, dframe)
  train.df[, fact] = NULL
}
```

# Start Predictions
```{r}
y = data.matrix(select(train.df, SalePrice))
```
```{r}
data.path  =  file.path(root, "input", "test.csv")
test.df = read.csv(data.path, header = TRUE)
test.df = select(test.df, -PoolQC, -MiscFeature, -Alley, -Fence)
ids = select(test.df, Id)
test.df = select(test.df, -Id)
```

```{r}
for( var in nas.meaningful){
  fact.levels = levels(test.df[, var])
  mode.val = "Meaningful.NA"
  test.df[var] = factor(test.df[,var], 
                  levels = c(fact.levels, mode.val))
  test.df[is.na(test.df[,var]), var] = mode.val
}
```
```{r}
for(var in nas.not.meaningful){
  if(! is.numeric(test.df[, var])){
    fact.levels = levels(test.df[, var])
    mode.val = sort(table(test.df[,var]), decreasing = T)[[1]]
    test.df[,var] = factor(test.df[,var], 
                  levels = c(fact.levels, mode.val))
  }
  else { 
    mode.val = median(test.df[,var], na.rm = T)
  }
    test.df[is.na(test.df[,var]), var] = mode.val
}
test.df$MSSubClass = as.factor(test.df$MSSubClass)
test.df$OverallQual = as.factor(test.df$OverallQual)
test.df$OverallCond = as.factor(test.df$OverallCond)
test.df$GarageYrBlt = NULL
for(bn in bad.names) {
  test.df[, bn] = NULL
}
test.df$YrSold = NULL
late.years = test.df$YearBuilt > 2000
test.df$late.years = late.years
```


Fill in missing data for test.df.

```{r}
mtable = colwise(function(x) sum(is.na(x)) > 0)(test.df)
missing.vars = names(test.df)[unlist(mtable)]
for(var in missing.vars){
  if(! is.numeric(test.df[, var])){
    fact.levels = levels(test.df[, var])
    mode.val = sort(table(test.df[,var]), decreasing = T)[[1]]
    test.df[,var] = factor(test.df[,var], 
                  levels = c(fact.levels, mode.val))
  }
  else { 
    mode.val = median(test.df[,var], na.rm = T)
  }
    test.df[is.na(test.df[,var]), var] = mode.val
}
```

```{r}
facts = colwise(is.factor)(test.df)
facts = names(test.df)[unlist(facts)]
for(fact in facts){
  nm = test.df[, fact] %>% table 
  nm = nm[nm > 0] %>% names
  nm = paste(fact, nm, sep="")
  dframe = data.frame(dummy(test.df [ ,fact]))
  names(dframe) = nm
  test.df = cbind(test.df, dframe)
  test.df[, fact] = NULL
}
```

# Predictions
```{r}
train.names = names(train.df)
train.names = train.names[train.names %in% names(test.df)]
nnx = names(test.df)
train.names = nnx[nnx %in% train.names]

train.df.new = train.df[, train.names]
test.df.new = test.df[, train.names]

X.test = data.matrix(test.df.new)
```

```{r}
X = data.matrix(train.df.new)
mod = cv.glmnet(X,y, alpha = 1)

preds = predict(mod, X.test)

pred.df = cbind(ids, preds)
names(pred.df) = c("Id", "SalePrice")
write.csv(pred.df, file = "output.csv", row.names = FALSE)